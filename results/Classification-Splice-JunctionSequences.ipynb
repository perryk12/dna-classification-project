{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the UCI Molecular Biology Splice-Junction Sequences dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                                                      EI\n",
      "id                                           ATRINS-DONOR-521\n",
      "Sequence                   CCAGCTGCATCACAGGAGGCCAGCGAGCAGG...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "url2 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/splice-junction-gene-sequences/splice.data\"\n",
    "names = ['Class', 'id', 'Sequence']\n",
    "data = pd.read_csv(url2, names = names)\n",
    "print(data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>...</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "      <td>3190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>876</td>\n",
       "      <td>858</td>\n",
       "      <td>876</td>\n",
       "      <td>884</td>\n",
       "      <td>865</td>\n",
       "      <td>898</td>\n",
       "      <td>858</td>\n",
       "      <td>878</td>\n",
       "      <td>909</td>\n",
       "      <td>849</td>\n",
       "      <td>...</td>\n",
       "      <td>874</td>\n",
       "      <td>874</td>\n",
       "      <td>934</td>\n",
       "      <td>859</td>\n",
       "      <td>838</td>\n",
       "      <td>908</td>\n",
       "      <td>870</td>\n",
       "      <td>860</td>\n",
       "      <td>953</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2     3     4     5     6     7     8     9  ...    51  \\\n",
       "count   3190  3190  3190  3190  3190  3190  3190  3190  3190  3190  ...  3190   \n",
       "unique     5     5     4     4     4     4     4     4     4     4  ...     5   \n",
       "top        G     C     C     C     C     C     C     C     C     T  ...     G   \n",
       "freq     876   858   876   884   865   898   858   878   909   849  ...   874   \n",
       "\n",
       "          52    53    54    55    56    57    58    59 Class  \n",
       "count   3190  3190  3190  3190  3190  3190  3190  3190  3190  \n",
       "unique     5     5     5     5     5     5     5     5     3  \n",
       "top        G     G     G     G     G     C     C     G     N  \n",
       "freq     874   934   859   838   908   870   860   953  1655  \n",
       "\n",
       "[4 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = data.loc[:, 'Class']\n",
    "# generate list of DNA sequences\n",
    "sequences = list(data.loc[:, 'Sequence'])\n",
    "dataset = {}\n",
    "\n",
    "# loop through sequences and split into individual nucleotides\n",
    "for i, seq in enumerate(sequences):\n",
    "    \n",
    "    # split into nucleotides, remove tab characters\n",
    "    nucleotides = list(seq)\n",
    "    nucleotides = [x for x in nucleotides if x != '\\t']\n",
    "    \n",
    "    # append class assignment\n",
    "    nucleotides.append(classes[i])\n",
    "    \n",
    "    # add to dataset\n",
    "    dataset[i] = nucleotides\n",
    "\n",
    "# get rid of spaces which make arrays different lengths\n",
    "for i in range(len(dataset)):\n",
    "    j = 0\n",
    "    while j < len(dataset[i]):\n",
    "        if dataset[i][j] == ' ':\n",
    "            dataset[i].remove(' ')\n",
    "            j-=1\n",
    "        j+=1\n",
    "        \n",
    "# turn dataset into pandas DataFrame\n",
    "dframe = pd.DataFrame(dataset)\n",
    "# transpose the DataFrame\n",
    "df = dframe.transpose()\n",
    "# for clarity, lets rename the last dataframe column to class\n",
    "df.rename(columns = {60: 'Class'}, inplace = True) \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0_A  0_C  0_D  0_G  0_T  1_A  1_C  1_D  1_G  1_T  ...  58_G  58_N  58_T  \\\n",
      "0    0    1    0    0    0    0    1    0    0    0  ...     0     0     1   \n",
      "1    1    0    0    0    0    0    0    0    1    0  ...     1     0     0   \n",
      "2    0    0    0    1    0    1    0    0    0    0  ...     0     0     1   \n",
      "3    0    0    0    1    0    0    0    0    1    0  ...     0     0     0   \n",
      "4    0    0    0    1    0    0    1    0    0    0  ...     0     0     0   \n",
      "\n",
      "   59_A  59_C  59_G  59_N  59_T  Class  Class_IE  \n",
      "0     0     0     1     0     0      1         0  \n",
      "1     0     1     0     0     0      1         0  \n",
      "2     0     0     1     0     0      1         0  \n",
      "3     0     1     0     0     0      1         0  \n",
      "4     0     0     0     0     1      1         0  \n",
      "\n",
      "[5 rows x 289 columns]\n"
     ]
    }
   ],
   "source": [
    "# convert strings to number data\n",
    "numerical_df = pd.get_dummies(df)\n",
    "df = numerical_df.drop(columns=['Class_IE'])\n",
    "df = numerical_df.drop(columns=['Class_N'])\n",
    "\n",
    "df.rename(columns = {'Class_EI': 'Class'}, inplace = True)\n",
    "print(df.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model_selection module to separate training and testing datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Create X and Y datasets for training\n",
    "X = np.array(df.drop(['Class'], 1))\n",
    "y = np.array(df['Class'])\n",
    "\n",
    "# define seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "# split data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: 0.881688 (0.020139)\n",
      "Test--  Nearest Neighbors :  0.8834586466165414\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       607\n",
      "           1       0.69      0.92      0.79       191\n",
      "\n",
      "    accuracy                           0.88       798\n",
      "   macro avg       0.83      0.89      0.85       798\n",
      "weighted avg       0.90      0.88      0.89       798\n",
      "\n",
      "Decision Tree: 0.983278 (0.006745)\n",
      "Test--  Decision Tree :  0.981203007518797\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       607\n",
      "           1       0.94      0.98      0.96       191\n",
      "\n",
      "    accuracy                           0.98       798\n",
      "   macro avg       0.97      0.98      0.97       798\n",
      "weighted avg       0.98      0.98      0.98       798\n",
      "\n",
      "Random Forest: 0.759217 (0.025522)\n",
      "Test--  Random Forest :  0.7606516290726817\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       607\n",
      "           1       0.00      0.00      0.00       191\n",
      "\n",
      "    accuracy                           0.76       798\n",
      "   macro avg       0.38      0.50      0.43       798\n",
      "weighted avg       0.58      0.76      0.66       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net: 0.981189 (0.006250)\n",
      "Test--  Neural Net :  0.9837092731829574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       607\n",
      "           1       0.94      0.99      0.97       191\n",
      "\n",
      "    accuracy                           0.98       798\n",
      "   macro avg       0.97      0.99      0.98       798\n",
      "weighted avg       0.98      0.98      0.98       798\n",
      "\n",
      "AdaBoost: 0.984531 (0.007486)\n",
      "Test--  AdaBoost :  0.9837092731829574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       607\n",
      "           1       0.94      0.99      0.97       191\n",
      "\n",
      "    accuracy                           0.98       798\n",
      "   macro avg       0.97      0.99      0.98       798\n",
      "weighted avg       0.98      0.98      0.98       798\n",
      "\n",
      "Naive Bayes: 0.965720 (0.006684)\n",
      "Test--  Naive Bayes :  0.9649122807017544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       607\n",
      "           1       0.88      0.99      0.93       191\n",
      "\n",
      "    accuracy                           0.96       798\n",
      "   macro avg       0.94      0.97      0.95       798\n",
      "weighted avg       0.97      0.96      0.97       798\n",
      "\n",
      "SVM Linear: 0.971571 (0.008113)\n",
      "Test--  SVM Linear :  0.968671679197995\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       607\n",
      "           1       0.90      0.98      0.94       191\n",
      "\n",
      "    accuracy                           0.97       798\n",
      "   macro avg       0.95      0.97      0.96       798\n",
      "weighted avg       0.97      0.97      0.97       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF: 0.984535 (0.007932)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test--  SVM RBF :  0.9824561403508771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       607\n",
      "           1       0.94      0.98      0.96       191\n",
      "\n",
      "    accuracy                           0.98       798\n",
      "   macro avg       0.97      0.98      0.98       798\n",
      "weighted avg       0.98      0.98      0.98       798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Sigmoid: 0.980771 (0.005660)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaper\\python\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test--  SVM Sigmoid :  0.9799498746867168\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       607\n",
      "           1       0.95      0.97      0.96       191\n",
      "\n",
      "    accuracy                           0.98       798\n",
      "   macro avg       0.97      0.98      0.97       798\n",
      "weighted avg       0.98      0.98      0.98       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# define scoring method\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Define models to train\n",
    "names = [\"Nearest Neighbors\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"SVM Linear\", \"SVM RBF\", \"SVM Sigmoid\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors = 3),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(kernel = 'linear'), \n",
    "    SVC(kernel = 'rbf'),\n",
    "    SVC(kernel = 'sigmoid')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state = seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Test-- ',name,': ',accuracy_score(y_test, predictions))\n",
    "    print()\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load partSix.py\n",
    "# Neural Networks Demystified\n",
    "# Part 6: Training\n",
    "#\n",
    "# Supporting code for short YouTube series on artificial neural networks.\n",
    "#\n",
    "# Stephen Welch\n",
    "# @stephencwelch\n",
    "\n",
    "\n",
    "## ----------------------- Part 1 ---------------------------- ##\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## ----------------------- Part 5 ---------------------------- ##\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, layer_i = 2, layer_o = 1, layer_h = 3):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = layer_i\n",
    "        self.outputLayerSize = layer_o\n",
    "        self.hiddenLayerSize = layer_h\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "\n",
    "def computeNumericalGradient(N, X, y):\n",
    "        paramsInitial = N.getParams()\n",
    "        numgrad = np.zeros(paramsInitial.shape)\n",
    "        perturb = np.zeros(paramsInitial.shape)\n",
    "        e = 1e-4\n",
    "\n",
    "        for p in range(len(paramsInitial)):\n",
    "            #Set perturbation vector\n",
    "            perturb[p] = e\n",
    "            N.setParams(paramsInitial + perturb)\n",
    "            loss2 = N.costFunction(X, y)\n",
    "            \n",
    "            N.setParams(paramsInitial - perturb)\n",
    "            loss1 = N.costFunction(X, y)\n",
    "\n",
    "            #Compute Numerical Gradient\n",
    "            numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "\n",
    "            #Return the value we changed to zero:\n",
    "            perturb[p] = 0\n",
    "            \n",
    "        #Return Params to original value:\n",
    "        N.setParams(paramsInitial)\n",
    "\n",
    "        return numgrad \n",
    "        \n",
    "## ----------------------- Part 6 ---------------------------- ##\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = model_selection.train_test_split(X, y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 288)\n",
      "(2392,)\n",
      "(798,)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "train_labels = train_labels/10\n",
    "test_labels = test_labels/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 1)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_labels.reshape(2392,1)\n",
    "test_labels = test_labels.reshape(798, 1)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.186488\n",
      "         Iterations: 12\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n"
     ]
    }
   ],
   "source": [
    "#Run the training. \n",
    "NN = Neural_Network(288, 1, 1)\n",
    "T = trainer(NN)\n",
    "T.train(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data error 0.0018281674701627558\n",
      "Testing Data error 0.0018206284369872343\n"
     ]
    }
   ],
   "source": [
    "pred_labels = NN.forward(train_vectors)\n",
    "\n",
    "print(\"Training Data error\", np.sum((train_labels - pred_labels)*(train_labels-pred_labels))/len(train_vectors))\n",
    "pred_labels = NN.forward(test_vectors)\n",
    "\n",
    "print(\"Testing Data error\", np.sum((test_labels - pred_labels)*(test_labels-pred_labels))/len(test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-- Artificial Neural Network: 0.9981793715630127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print('Test-- Artificial Neural Network:', 1 - np.sum((test_labels - pred_labels)*(test_labels-pred_labels))/len(test_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.180720\n",
      "         Iterations: 37\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 40\n",
      "Training Data error 0.0018233444445860927\n",
      "Testing Data error 0.0018202570728444803\n"
     ]
    }
   ],
   "source": [
    "#Run the training. \n",
    "NN = Neural_Network(288, 1, 2)\n",
    "T = trainer(NN)\n",
    "T.train(train_vectors, train_labels)\n",
    "\n",
    "pred_labels = NN.forward(train_vectors)\n",
    "print(\"Training Data error\", np.sum((train_labels - pred_labels)*(train_labels-pred_labels))/len(train_vectors))\n",
    "pred_labels = NN.forward(test_vectors)\n",
    "print(\"Testing Data error\", np.sum((test_labels - pred_labels)*(test_labels-pred_labels))/len(test_vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print('Test-- Artificial Neural Network:', 1 - np.sum((test_labels - pred_labels)*(test_labels-pred_labels))/len(test_vectors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
